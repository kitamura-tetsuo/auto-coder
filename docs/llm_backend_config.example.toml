# Auto-Coder LLM Backend Configuration
# This file configures the LLM backends used by the application
#
# Configuration location: ~/.auto-coder/llm_config.toml

[backend]
# Default backend to use for LLM operations (default: codex)
default = "gemini"

# Order in which to try backends for LLM operations
# If not specified, all enabled backends will be used in alphabetical order
order = ["gemini", "qwen", "auggie", "claude", "codex", "codex-mcp"]

[message_backend]
# Default backend for message generation (commit messages, PR messages, etc.)
default = "qwen"

# Order in which to try backends for message generation
# If not specified, will fall back to the general backend order
order = ["qwen", "gemini", "auggie"]

# Individual backend configurations
[backends.gemini]
enabled = true
model = "gemini-2.5-pro"
api_key = "your-gemini-api-key"  # Optional: can be set via GEMINI_API_KEY env var

[backends.qwen]
enabled = true
model = "qwen3-coder-plus"
openai_api_key = "your-openai-api-key"  # Optional: can be set via OPENAI_API_KEY env var
openai_base_url = "https://api.openai.com/v1"  # Optional: can be set via OPENAI_BASE_URL env var

[backends.auggie]
enabled = true
model = "GPT-5"

[backends.claude]
enabled = true
model = "sonnet"

[backends.codex]
enabled = true
model = "codex"

[backends.codex-mcp]
enabled = true
model = "codex-mcp"