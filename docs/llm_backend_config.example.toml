# LLM Backend Configuration Example
# =================================
#
# This file demonstrates how to configure LLM backends for Auto-Coder.
# Save this file as `~/.auto-coder/llm_config.toml` or specify a custom path.

[backend]
# Default backend to use when multiple are available
default = "qwen"

# Order of backends to try (only enabled backends will be used)
order = ["qwen", "gemini", "codex", "claude"]

[message_backend]
# Separate configuration for message generation (commit messages, PR descriptions)
# If not specified, falls back to general backend configuration
default = "qwen"
order = ["qwen", "gemini"]

[backends]

# Qwen Configuration
# ------------------
# Qwen client uses the 'qwen' CLI and authenticates via OAuth. No API keys needed.
[qwen]
enabled = true
model = "qwen3-coder-plus"

# Additional options to pass to the Qwen CLI
# Example: ["-o", "stream", "false", "--debug"]
options = []

# Backend type identifier
backend_type = "qwen"

# Gemini Configuration
# --------------------
[gemini]
enabled = true
model = "gemini-2.5-pro"

# Gemini API settings
api_key = "your-gemini-api-key-here"
base_url = "https://generativelanguage.googleapis.com"

# Optional retry configuration for usage limits
usage_limit_retry_count = 3
usage_limit_retry_wait_seconds = 30
# Rotate to the next backend after every successful call to stay under strict execution limits
always_switch_after_execution = true

backend_type = "gemini"

# Codex Configuration
# -------------------
# Codex client can be used for generic OpenAI-compatible APIs like OpenRouter.
[codex]
enabled = true
model = "codex" # Placeholder, model is usually set by the provider
# Generic API key and base URL for providers that don't use OpenAI-specific env vars
api_key = "your-api-key-here"
base_url = "your-api-base-url-here"
# For OpenAI-compatible APIs (like OpenRouter), use these
openai_api_key = "your-openai-api-key-here"
openai_base_url = "your-openai-base-url-here"
options = []
backend_type = "codex"

# --- Custom Alias Examples ---

# OpenRouter with Codex Backend Type
# ----------------------------------
# This example shows how to configure a generic OpenRouter model using `backend_type = "codex"`.
# This allows you to use any model from OpenRouter that is OpenAI-compatible.
[openrouter-grok]
enabled = true
model = "groq/llama3-8b-8192" # The model identifier from OpenRouter
backend_type = "codex" # Use the codex client for this
openai_api_key = "sk-or-v1-your-openrouter-key"
openai_base_url = "https://openrouter.ai/api/v1"
# You can add custom headers required by the provider in the options
options = ["-o", "HTTPReferer", "https://yourapp.com", "-o", "XTitle", "Auto-Coder"]

# Custom Qwen Alias
# -----------------
[qwen-fast]
enabled = true
model = "qwen-turbo"
backend_type = "qwen"
options = ["-o", "timeout", "30", "-o", "stream", "true"]


# Notes:
# ------
# 1. The `options` field is used by `qwen` and `codex` backends to pass
#    command-line arguments to their respective CLI tools.
#
# 2. The `backend_type` field allows you to create custom aliases that
#    map to underlying backend implementations (`qwen`, `gemini`, `codex`, `claude`).
#
# 3. Environment variables can override configuration values:
#    - AUTO_CODER_DEFAULT_BACKEND
#    - AUTO_CODER_MESSAGE_DEFAULT_BACKEND
#    - AUTO_CODER_<BACKEND>_API_KEY (e.g., AUTO_CODER_GEMINI_API_KEY)
#    - AUTO_CODER_<BACKEND>_OPENAI_API_KEY (e.g., AUTO_CODER_CODEX_OPENAI_API_KEY)
#    - AUTO_CODER_<BACKEND>_OPENAI_BASE_URL

