# LLM Backend Configuration Example
# =================================
#
# This file demonstrates how to configure LLM backends for Auto-Coder.
# Save this file as `~/.auto-coder/llm_config.toml` or specify a custom path.

[backend]
# Default backend to use when multiple are available
default = "qwen-openrouter"

# Order of backends to try (only enabled backends will be used)
order = ["qwen-openrouter", "qwen", "gemini", "codex", "claude"]

[backend_for_noedit]
# Separate configuration for non-editing operations (commit messages, PR descriptions)
# If not specified, falls back to general backend configuration
default = "qwen-openrouter"
order = ["qwen-openrouter", "qwen", "gemini"]

[backends]

# Backend Configuration
# =====================
# Note: enabled = true is the default behavior for all backends.
# Only specify "enabled = false" to disable a backend.
# Only the following backends are disabled by default (optional services):
# - qwen-azure: Azure OpenAI (optional)
# - qwen-custom: Custom OpenAI-compatible endpoint (optional)

# Qwen Configuration - Two Methods
# =================================

# Method 1: Qwen CLI (OAuth) - Direct Qwen Usage
# ----------------------------------------------
# Use backend_type = "qwen" for native Qwen CLI authentication
[qwen]
# enabled = true is the default (no need to specify)
model = "qwen3-coder-plus"

# No API keys needed - uses OAuth with Qwen CLI
# Note: This requires Qwen CLI to be installed and authenticated

# Backend type identifier - "qwen" for native CLI
backend_type = "qwen"

# Additional options to pass to the Qwen CLI
# These options will be added to the command line when calling qwen
# Example: ["-o", "stream", "false", "--debug"]
#
# options: Used for code editing operations (default LLM operations)
# options_for_noedit: Used for message generation (commit messages, PR descriptions)
#
# For most backends, you can use the same options for both or leave empty to use defaults.
# The system automatically adds required flags (e.g., -y for qwen) during execution.
options = []
options_for_noedit = []


# Method 2: Qwen via OpenRouter (OpenAI-Compatible API)
# -----------------------------------------------------
# Use backend_type = "codex" for OpenAI-compatible providers
[qwen-openrouter]
# enabled = true is the default (no need to specify)
model = "qwen/qwen3-coder:free"

# OpenAI-compatible API settings (required for OpenRouter, Azure, etc.)
openai_api_key = "sk-or-v1-your-openrouter-key-here"
openai_base_url = "https://openrouter.ai/api/v1"

# Backend type identifier - "codex" for OpenAI-compatible client
backend_type = "codex"

# Optional: Add custom headers for tracking usage
# options: Used for code editing operations
# options_for_noedit: Used for message generation (commit messages, PR descriptions)
options = ["-o", "HTTPReferer", "https://yourapp.com", "-o", "XTitle", "Auto-Coder"]
options_for_noedit = ["-o", "HTTPReferer", "https://yourapp.com", "-o", "XTitle", "Auto-Coder"]

# Gemini Configuration
# --------------------
[gemini]
# enabled = true is the default (no need to specify)
model = "gemini-2.5-pro"

# Gemini API settings
api_key = "your-gemini-api-key-here"
base_url = "https://generativelanguage.googleapis.com"

# Optional retry configuration for usage limits
usage_limit_retry_count = 3
usage_limit_retry_wait_seconds = 30
# Rotate to the next backend after every successful call to stay under strict execution limits
always_switch_after_execution = true

backend_type = "gemini"

# Optional: Additional CLI options
# options: Used for code editing operations
# options_for_noedit: Used for message generation (commit messages, PR descriptions)
# Note: The system automatically adds --yolo and --force-model flags during execution
options = []
options_for_noedit = []

# Codex Configuration
# -------------------
# Codex client can be used for generic OpenAI-compatible APIs like OpenRouter.
[codex]
# enabled = true is the default (no need to specify)
model = "codex" # Placeholder, model is usually set by the provider
# Generic API key and base URL for providers that don't use OpenAI-specific env vars
api_key = "your-api-key-here"
base_url = "your-api-base-url-here"
# For OpenAI-compatible APIs (like OpenRouter), use these
openai_api_key = "your-openai-api-key-here"
openai_base_url = "your-openai-base-url-here"

# Optional: Additional CLI options
# options: Used for code editing operations
# options_for_noedit: Used for message generation (commit messages, PR descriptions)
# Note: The system automatically adds --dangerously-bypass-approvals-and-sandbox during execution
options = []
options_for_noedit = []

backend_type = "codex"

# Custom Alias Examples
# --------------------
# You can create custom aliases that point to underlying backend types
# This allows you to have multiple configurations for the same backend

# Example 1: Parent backend with common OpenRouter configuration
# ================================================================
# This parent defines common options that child backends can inherit
[openrouter-base]
# enabled = true is the default (no need to specify)
model = "qwen/qwen-2.5-plus"
openai_api_key = "sk-or-v1-your-openrouter-key-here"
openai_base_url = "https://openrouter.ai/api/v1"

# Use 'codex' backend type (OpenAI-compatible)
backend_type = "codex"

# Common options for all OpenRouter backends
# options: Used for code editing operations
# options_for_noedit: Used for message generation (commit messages, PR descriptions)
# Child backends can inherit these options automatically!
options = ["-o", "HTTPReferer", "https://yourapp.com", "-o", "XTitle", "Auto-Coder"]
options_for_noedit = ["-o", "HTTPReferer", "https://yourapp.com", "-o", "XTitle", "Auto-Coder"]


# Example 2: Child backend inheriting from parent
# ===============================================
# This child inherits options from openrouter-base
[qwen-fast]
# enabled = true is the default (no need to specify)
model = "qwen/qwen-2.5-plus"

# Inherit backend_type and options from openrouter-base!
backend_type = "openrouter-base"

# No need to specify options - automatically inherited from openrouter-base
# options = []  # <-- This would prevent inheritance! Don't add it unless you want to override


# Example 3: Child backend with custom model but inherited options
# ================================================================
[qwen-premium]
# enabled = true is the default (no need to specify)
model = "qwen/qwen-2.5-coder-72b-instruct"

# Inherit backend_type and options from openrouter-base!
backend_type = "openrouter-base"

# No need to specify options - automatically inherited from openrouter-base
# The premium model uses the same options as defined in openrouter-base


# Example 4: Azure OpenAI with Qwen model
# NOTE: Disabled by default (optional service)
[qwen-azure]
enabled = false
model = "qwen-35-coder"
openai_api_key = "your-azure-openai-key"
openai_base_url = "https://your-resource.openai.azure.com"

# Use 'codex' backend type for Azure OpenAI
backend_type = "codex"

# Azure-specific options
# options: Used for code editing operations
# options_for_noedit: Used for message generation (commit messages, PR descriptions)
options = ["-o", "api_version", "2024-02-01"]
options_for_noedit = ["-o", "api_version", "2024-02-01"]


# Example 5: Custom OpenAI-compatible endpoint
# NOTE: Disabled by default (optional service)
[qwen-custom]
enabled = false
model = "qwen-2.5-coder-32k-instruct"
openai_api_key = "your-custom-endpoint-key"
openai_base_url = "https://api.example.com/v1"

# Use 'codex' backend type for OpenAI-compatible providers
backend_type = "codex"

# Optional: Additional CLI options
# options: Used for code editing operations
# options_for_noedit: Used for message generation (commit messages, PR descriptions)
options = []
options_for_noedit = []


# Claude Configuration
# --------------------
# Example configuration for Claude backend
# [claude]
# # enabled = true is the default (no need to specify)
# model = "sonnet"
# backend_type = "claude"
#
# # Optional: Path to Claude settings file
# # settings = "/path/to/claude-settings.json"
#
# # Optional: Additional CLI options
# # options: Used for code editing operations
# # options_for_noedit: Used for message generation (commit messages, PR descriptions)
# # Note: The system automatically adds --print, --dangerously-skip-permissions,
# #       and --allow-dangerously-skip-permissions during execution
# options = []
# options_for_noedit = []


# Auggie Configuration
# --------------------
# Example configuration for Auggie backend (requires npm install -g @augmentcode/auggie)
# [auggie]
# # enabled = true is the default (no need to specify)
# model = "GPT-5"
# backend_type = "auggie"
#
# # Optional: Additional CLI options
# # options: Used for code editing operations
# # options_for_noedit: Used for message generation (commit messages, PR descriptions)
# # Note: The system automatically adds --print flag during execution
# options = []
# options_for_noedit = []


# Jules Configuration
# -------------------
# Example configuration for Jules backend (session-based AI assistant)
# [jules]
# # enabled = true is the default (no need to specify)
# backend_type = "jules"
#
# # Optional: Additional CLI options
# # Jules is session-based, so minimal options are typically needed
# # options: Used for code editing operations
# # options_for_noedit: Used for message generation (commit messages, PR descriptions)
# options = []
# options_for_noedit = []


# Fallback Backend for Failed PRs
# --------------------------------
# The `backend_for_failed_pr` section allows you to specify a fallback backend
# to use when PR processing fails. This is useful for resilience and ensuring
# that PR processing can continue even when the default backend is unavailable.
#
# If a PR fails to be processed by the primary backends, the system will
# automatically try the fallback backend specified in this section.
#
# Example configuration:
# [backend_for_failed_pr]
# # Backend should be enabled (default: true)
# enabled = true
#
# # Model to use for the fallback backend
# model = "gemini-2.5-flash"
#
# # API key (can also be set via environment variable)
# api_key = "your-fallback-api-key"
#
# # Optional: Override default temperature (0.0 to 1.0)
# temperature = 0.2
#
# # Optional: Override default timeout (in seconds)
# timeout = 120
#
# # Optional: Backend type (e.g., "codex", "gemini", "qwen")
# backend_type = "gemini"
#
# # Optional: List of providers for this backend
# providers = ["google"]
#
# # Optional: OpenAI-compatible configuration
# openai_api_key = "your-openai-key"
# openai_base_url = "https://api.openai.com/v1"
#
# # Optional: Usage limit retry configuration
# usage_limit_retry_count = 3
# usage_limit_retry_wait_seconds = 30
#
# # Optional: Additional options
# options = ["stream", "json_mode"]
#
# # Optional: Model provider (e.g., "openrouter", "anthropic", "google")
# model_provider = "google"
#
# # Optional: Always switch to next backend after execution
# always_switch_after_execution = false
#
# # Optional: Path to settings file (for Claude backend)
# settings = "path/to/settings.json"
#
# # Optional: Usage markers for tracking
# usage_markers = ["fallback", "high_availability"]


# Understanding Options: options vs options_for_noedit
# =====================================================
#
# The configuration system supports two types of option fields for each backend:
#
# 1. `options` - Used for code editing operations (default LLM operations)
#    - Applied when the LLM is called to analyze issues, generate code, fix bugs, etc.
#    - These are the primary operations where the LLM modifies files
#
# 2. `options_for_noedit` - Used for message generation operations
#    - Applied when generating commit messages, PR descriptions, and other text
#    - Allows different configurations for non-code operations
#    - If not specified, falls back to using the same options as `options`
#
# IMPORTANT: Automatic Flag Management
# -------------------------------------
# The system automatically adds required CLI flags during execution:
# - Codex: --dangerously-bypass-approvals-and-sandbox (for exec mode)
# - Claude: --print, --dangerously-skip-permissions, --allow-dangerously-skip-permissions
# - Gemini: --yolo, --force-model
# - Qwen: -y (auto-confirm)
#
# You do NOT need to include these flags in your options configuration.
# Only add additional custom options beyond the defaults.
#
# Migration from Old Structure
# -----------------------------
# Prior to this configuration schema, CLI options were hardcoded in client implementations.
# The new structure allows full customization through configuration:
#
# OLD (hardcoded in code):
#   - Codex always used: --dangerously-bypass-approvals-and-sandbox
#   - Claude always used: --print --dangerously-skip-permissions --allow-dangerously-skip-permissions
#   - Gemini always used: --yolo --force-model
#   - Qwen always used: -y
#
# NEW (configurable):
#   - System adds required flags automatically
#   - You can add additional flags via `options` and `options_for_noedit`
#   - Example: options = ["-o", "timeout", "30"] to customize timeout
#
# Common Use Cases
# ----------------
# 1. Same options for all operations:
#    options = ["--debug"]
#    options_for_noedit = ["--debug"]
#
# 2. Different options for editing vs message generation:
#    options = ["--verbose", "-o", "timeout", "120"]
#    options_for_noedit = ["-o", "timeout", "30"]  # Faster for messages
#
# 3. Using defaults (empty arrays):
#    options = []
#    options_for_noedit = []
#
# 4. OpenRouter tracking headers (for codex-based backends):
#    options = ["-o", "HTTPReferer", "https://yourapp.com", "-o", "XTitle", "Auto-Coder"]
#    options_for_noedit = ["-o", "HTTPReferer", "https://yourapp.com", "-o", "XTitle", "Auto-Coder"]

# Notes and Best Practices
# ========================

# IMPORTANT: backend_type Configuration
# ---------------------------------------
# The `backend_type` field is crucial for proper configuration:
#
# 1. "qwen" - Use native Qwen CLI (OAuth authentication)
#    - No API keys required
#    - Requires Qwen CLI installation and authentication
#    - Limited to models available through Qwen CLI
#
# 2. "codex" - Use OpenAI-compatible client (API keys required)
#    - Required for OpenRouter, Azure OpenAI, and similar providers
#    - Requires openai_api_key and openai_base_url
#    - Access to all models offered by the provider
#
# 3. "gemini" - Use Gemini client
#    - Requires api_key and base_url
#    - Direct Google API integration
#
# 4. "claude" - Use Claude client
#    - Anthropic API integration
#
# 5. "jules" - Use Jules AI assistant
#    - Session-based AI assistant
#    - Requires Jules CLI installation and authentication
#    - Integrated with cloud.csv for session management
#
# 6. "codex" (for default codex backend) - Use Codex client
#    - Default OpenAI Codex integration

# Backend Aliases
# ---------------
# You can create multiple configurations for the same backend type by using
# different backend names with the same backend_type. This is useful for:
# - Using multiple models from the same provider
# - Different configurations for different use cases
# - Fallback configurations with different models

# Fallback Backend Strategy
# --------------------------
# The `backend_for_failed_pr` section provides a strategic fallback mechanism:
# 1. When a PR fails to be processed by the primary backends
# 2. The system automatically attempts to process using the fallback backend
# 3. This ensures high availability and resilience in PR automation
# 4. The fallback backend can use a different model or provider for redundancy

# Configuration Inheritance and Options
# -------------------------------------
# 1. The `options` field is primarily used by qwen/codex backends to pass
#    command-line arguments to the CLI tools.
#
# 2. Only qwen and codex backends currently utilize the `options` field in their
#    subprocess calls. Other backends may ignore this field.
#
# 3. The `backend_type` field allows you to create custom aliases that
#    map to underlying backend implementations. This is useful when you
#    want multiple configurations for the same backend type.

# Backend Inheritance (Important!)
# ---------------------------------
# The configuration system supports option inheritance from parent backends.
# This allows you to define common options once and reuse them across multiple backends.
#
# How It Works:
# 1. Define a parent backend with common options
# 2. Create child backends that specify `backend_type = "parent_name"`
# 3. Child backends automatically inherit `options` and `options_for_noedit` from the parent
# 4. Inheritance ONLY occurs when child backends do NOT explicitly define these fields
# 5. If a child explicitly sets `options` (even to an empty list []), no inheritance occurs
#
# Example - Parent with common options:
# [openrouter-base]
# backend_type = "codex"
# model = "qwen/qwen-2.5-plus"
# openai_api_key = "sk-or-v1-your-openrouter-key"
# openai_base_url = "https://openrouter.ai/api/v1"
# options = ["-o", "HTTPReferer", "https://yourapp.com", "-o", "XTitle", "Auto-Coder"]
# options_for_noedit = ["-o", "timeout", "30"]
#
# Child inherits from parent:
# [qwen-fast]
# backend_type = "openrouter-base"  # Inherits options from openrouter-base
# model = "qwen/qwen-2.5-plus"
#
# Child overrides parent:
# [qwen-premium]
# backend_type = "openrouter-base"
# model = "qwen/qwen-2.5-coder-72b"
# options = ["-o", "timeout", "120"]  # Explicitly set - NO inheritance!
#
# Key Rules:
# - Parent backends are identified by their name
# - Child backends use `backend_type = "parent_name"` to inherit
# - Options are copied (not referenced) - modifying child options won't affect parent
# - Only immediate parent inheritance is supported (no multi-level chaining)
# - Inheritance happens automatically during config loading
# - Explicit configuration always takes precedence over inheritance

# Environment Variables
# --------------------
# Environment variables can override configuration values:
#    - AUTO_CODER_DEFAULT_BACKEND
#    - AUTO_CODER_NOEDIT_DEFAULT_BACKEND
#    - AUTO_CODER_<BACKEND>_API_KEY (e.g., AUTO_CODER_GEMINI_API_KEY)
#    - AUTO_CODER_<BACKEND>_OPENAI_API_KEY (e.g., AUTO_CODER_CODEX_OPENAI_API_KEY)
#    - AUTO_CODER_<BACKEND>_OPENAI_BASE_URL

# Provider-Specific Configurations
# --------------------------------
# 1. OpenAI-compatible providers (OpenRouter, Azure OpenAI, custom endpoints)
#    should use:
#    - openai_api_key
#    - openai_base_url
#    - backend_type = "codex"
#
# 2. Native providers (Gemini, Claude)
#    should use:
#    - api_key
#    - base_url
#    - backend_type matching the provider name
