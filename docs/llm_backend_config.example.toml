# LLM Backend Configuration Example
# =================================
#
# This file demonstrates how to configure LLM backends for Auto-Coder.
# Save this file as `~/.auto-coder/llm_config.toml` or specify a custom path.

[backend]
# Default backend to use when multiple are available
default = "qwen-openrouter"

# Order of backends to try (only enabled backends will be used)
order = ["qwen-openrouter", "qwen", "gemini", "codex", "claude"]

[backend_for_noedit]
# Separate configuration for non-editing operations (message generation, commit messages, PR descriptions)
# If not specified, falls back to general backend configuration
# Note: This was previously called [message_backend] in older versions
default = "qwen-openrouter"
order = ["qwen-openrouter", "qwen", "gemini"]

[backends]

# Backend Configuration
# =====================
# Note: enabled = true is the default behavior for all backends.
# Only specify "enabled = false" to disable a backend.
# Only the following backends are disabled by default (optional services):
# - qwen-azure: Azure OpenAI (optional)
# - qwen-custom: Custom OpenAI-compatible endpoint (optional)

# Qwen Configuration - Two Methods
# =================================

# Method 1: Qwen CLI (OAuth) - Direct Qwen Usage
# ----------------------------------------------
# Use backend_type = "qwen" for native Qwen CLI authentication
[qwen]
# enabled = true is the default (no need to specify)
model = "qwen3-coder-plus"

# No API keys needed - uses OAuth with Qwen CLI
# Note: This requires Qwen CLI to be installed and authenticated

# Backend type identifier - "qwen" for native CLI
backend_type = "qwen"

# Additional options to pass to the Qwen CLI
# These options will be added to the command line when calling qwen
# Example: ["-o", "stream", "false", "--debug"]
options = ["-y"]

# Options for non-editing operations (message generation, commit messages, PR descriptions)
# If not specified, defaults to the same as `options`
options_for_noedit = ["-y"]


# Method 2: Qwen via OpenRouter (OpenAI-Compatible API)
# -----------------------------------------------------
# Use backend_type = "codex" for OpenAI-compatible providers
[qwen-openrouter]
# enabled = true is the default (no need to specify)
model = "qwen/qwen3-coder:free"

# OpenAI-compatible API settings (required for OpenRouter, Azure, etc.)
openai_api_key = "sk-or-v1-your-openrouter-key-here"
openai_base_url = "https://openrouter.ai/api/v1"

# Backend type identifier - "codex" for OpenAI-compatible client
backend_type = "codex"

# Optional: Add custom headers for tracking usage
options = ["-o", "HTTPReferer", "https://yourapp.com", "-o", "XTitle", "Auto-Coder"]

# Options for non-editing operations (message generation, commit messages, PR descriptions)
# If not specified, defaults to the same as `options`
options_for_noedit = ["-o", "HTTPReferer", "https://yourapp.com", "-o", "XTitle", "Auto-Coder"]

# Gemini Configuration
# --------------------
[gemini]
# enabled = true is the default (no need to specify)
model = "gemini-2.5-pro"

# Gemini API settings
api_key = "your-gemini-api-key-here"
base_url = "https://generativelanguage.googleapis.com"

# Optional retry configuration for usage limits
usage_limit_retry_count = 3
usage_limit_retry_wait_seconds = 30
# Rotate to the next backend after every successful call to stay under strict execution limits
always_switch_after_execution = true

backend_type = "gemini"

# Options for code editing operations
# These are passed to the backend when performing code modifications
options = ["--yolo", "--force-model"]

# Options for message generation (commit messages, PR descriptions)
# If not specified, defaults to the same as `options`
options_for_noedit = ["--yolo", "--force-model"]

# Codex Configuration
# -------------------
# Codex client can be used for generic OpenAI-compatible APIs like OpenRouter.
[codex]
# enabled = true is the default (no need to specify)
model = "codex" # Placeholder, model is usually set by the provider
# Generic API key and base URL for providers that don't use OpenAI-specific env vars
api_key = "your-api-key-here"
base_url = "your-api-base-url-here"
# For OpenAI-compatible APIs (like OpenRouter), use these
openai_api_key = "your-openai-api-key-here"
openai_base_url = "your-openai-base-url-here"

# Options for code editing operations
# These are passed to the backend when performing code modifications
options = ["--dangerously-bypass-approvals-and-sandbox"]

# Options for message generation (commit messages, PR descriptions)
# If not specified, defaults to the same as `options`
options_for_noedit = ["--dangerously-bypass-approvals-and-sandbox"]

backend_type = "codex"

# Claude Configuration
# --------------------
[claude]
# enabled = true is the default (no need to specify)
model = "sonnet"

# Claude API settings
api_key = "your-claude-api-key-here"

backend_type = "claude"

# Options for code editing operations
# These are passed to the backend when performing code modifications
options = ["--print", "--dangerously-skip-permissions", "--allow-dangerously-skip-permissions"]

# Options for message generation (commit messages, PR descriptions)
# If not specified, defaults to the same as `options`
options_for_noedit = ["--print", "--dangerously-skip-permissions", "--allow-dangerously-skip-permissions"]

# Auggie Configuration
# --------------------
[auggie]
# enabled = true is the default (no need to specify)
model = "GPT-5"

# Auggie API settings
api_key = "your-auggie-api-key-here"

backend_type = "auggie"

# Options for code editing operations
# These are passed to the backend when performing code modifications
options = ["--print"]

# Options for message generation (commit messages, PR descriptions)
# If not specified, defaults to the same as `options`
options_for_noedit = ["--print"]

# Jules Configuration
# --------------------
[jules]
# enabled = true is the default (no need to specify)
# Jules is session-based AI assistant
# No model needed - Jules manages its own models internally

backend_type = "jules"

# Jules is session-based, so minimal options are needed
# Options for code editing operations
options = []

# Options for message generation (commit messages, PR descriptions)
# If not specified, defaults to the same as `options`
options_for_noedit = []

# Custom Alias Examples
# --------------------
# You can create custom aliases that point to underlying backend types
# This allows you to have multiple configurations for the same backend

# Example 1: Qwen via OpenRouter with faster model
[qwen-fast]
# enabled = true is the default (no need to specify)
model = "qwen/qwen-2.5-plus"
openai_api_key = "sk-or-v1-your-openrouter-key-here"
openai_base_url = "https://openrouter.ai/api/v1"

# Use 'codex' backend type (OpenAI-compatible)
backend_type = "codex"

# Custom options for faster responses
options = ["-o", "timeout", "30", "-o", "stream", "true"]

# Options for message generation (commit messages, PR descriptions)
# If not specified, defaults to the same as `options`
options_for_noedit = ["-o", "timeout", "30", "-o", "stream", "true"]


# Example 2: Qwen via OpenRouter with premium model
[qwen-premium]
# enabled = true is the default (no need to specify)
model = "qwen/qwen-2.5-coder-72b-instruct"
openai_api_key = "sk-or-v1-your-openrouter-key-here"
openai_base_url = "https://openrouter.ai/api/v1"

# Use 'codex' backend type (OpenAI-compatible)
backend_type = "codex"

# Premium model with additional options
options = ["-o", "HTTPReferer", "https://yourapp.com", "-o", "XTitle", "Auto-Coder"]

# Options for message generation (commit messages, PR descriptions)
# If not specified, defaults to the same as `options`
options_for_noedit = ["-o", "HTTPReferer", "https://yourapp.com", "-o", "XTitle", "Auto-Coder"]


# Example 3: Azure OpenAI with Qwen model
# NOTE: Disabled by default (optional service)
[qwen-azure]
enabled = false
model = "qwen-35-coder"
openai_api_key = "your-azure-openai-key"
openai_base_url = "https://your-resource.openai.azure.com"

# Use 'codex' backend type for Azure OpenAI
backend_type = "codex"

# Azure-specific options
options = ["-o", "api_version", "2024-02-01"]

# Options for message generation (commit messages, PR descriptions)
# If not specified, defaults to the same as `options`
options_for_noedit = ["-o", "api_version", "2024-02-01"]


# Example 4: Custom OpenAI-compatible endpoint
# NOTE: Disabled by default (optional service)
[qwen-custom]
enabled = false
model = "qwen-2.5-coder-32k-instruct"
openai_api_key = "your-custom-endpoint-key"
openai_base_url = "https://api.example.com/v1"

# Use 'codex' backend type for OpenAI-compatible providers
backend_type = "codex"

# Optional options for custom endpoints
options = ["-o", "timeout", "60"]

# Options for message generation (commit messages, PR descriptions)
# If not specified, defaults to the same as `options`
options_for_noedit = ["-o", "timeout", "60"]

# Fallback Backend for Failed PRs
# --------------------------------
# The `backend_for_failed_pr` section allows you to specify a fallback backend
# to use when PR processing fails. This is useful for resilience and ensuring
# that PR processing can continue even when the default backend is unavailable.
#
# If a PR fails to be processed by the primary backends, the system will
# automatically try the fallback backend specified in this section.
#
# Example configuration:
# [backend_for_failed_pr]
# # Backend should be enabled (default: true)
# enabled = true
#
# # Model to use for the fallback backend
# model = "gemini-2.5-flash"
#
# # API key (can also be set via environment variable)
# api_key = "your-fallback-api-key"
#
# # Optional: Override default temperature (0.0 to 1.0)
# temperature = 0.2
#
# # Optional: Override default timeout (in seconds)
# timeout = 120
#
# # Optional: Backend type (e.g., "codex", "gemini", "qwen")
# backend_type = "gemini"
#
# # Optional: List of providers for this backend
# providers = ["google"]
#
# # Optional: OpenAI-compatible configuration
# openai_api_key = "your-openai-key"
# openai_base_url = "https://api.openai.com/v1"
#
# # Optional: Usage limit retry configuration
# usage_limit_retry_count = 3
# usage_limit_retry_wait_seconds = 30
#
# # Optional: Additional options
# options = ["stream", "json_mode"]
#
# # Optional: Model provider (e.g., "openrouter", "anthropic", "google")
# model_provider = "google"
#
# # Optional: Always switch to next backend after execution
# always_switch_after_execution = false
#
# # Optional: Path to settings file (for Claude backend)
# settings = "path/to/settings.json"
#
# # Optional: Usage markers for tracking
# usage_markers = ["fallback", "high_availability"]


# Understanding options vs options_for_noedit
# ============================================
#
# Auto-Coder distinguishes between two types of operations:
#
# 1. CODE EDITING OPERATIONS (options)
#    ---------------------------------
#    These options are used when the backend performs code modifications:
#    - Analyzing and implementing code changes
#    - Writing new files
#    - Refactoring existing code
#    - Fixing bugs in source files
#    - Running tests and builds
#
#    Example: ["--dangerously-bypass-approvals-and-sandbox"]
#
# 2. MESSAGE GENERATION OPERATIONS (options_for_noedit)
#    ---------------------------------------------------
#    These options are used for non-editing operations:
#    - Generating commit messages
#    - Writing PR descriptions
#    - Creating issue descriptions
#    - Writing documentation comments
#    - Any operation that doesn't modify code files
#
#    Example: ["--print", "--dangerously-skip-permissions"]
#
# MIGRATION NOTES:
# ================
# - The old hardcoded `options` field has been replaced with a more flexible system
# - You can now specify different options for editing vs message generation
# - If `options_for_noedit` is not specified, it defaults to the same value as `options`
# - The `[message_backend]` section has been renamed to `[backend_for_noedit]`
#   (backward compatibility is maintained, but the new name is recommended)
#
# BEST PRACTICES:
# ===============
# - Use `options` for code editing operations that require more aggressive settings
# - Use `options_for_noedit` for message generation that may need different options
# - Keep options minimal and focused on the specific operation type
# - Test your configuration changes in a safe environment first
# - Monitor logs to ensure options are being applied correctly


# Notes and Best Practices
# ========================

# IMPORTANT: backend_type Configuration
# ---------------------------------------
# The `backend_type` field is crucial for proper configuration:
#
# 1. "qwen" - Use native Qwen CLI (OAuth authentication)
#    - No API keys required
#    - Requires Qwen CLI installation and authentication
#    - Limited to models available through Qwen CLI
#
# 2. "codex" - Use OpenAI-compatible client (API keys required)
#    - Required for OpenRouter, Azure OpenAI, and similar providers
#    - Requires openai_api_key and openai_base_url
#    - Access to all models offered by the provider
#
# 3. "gemini" - Use Gemini client
#    - Requires api_key and base_url
#    - Direct Google API integration
#
# 4. "claude" - Use Claude client
#    - Anthropic API integration
#
# 5. "jules" - Use Jules AI assistant
#    - Session-based AI assistant
#    - Requires Jules CLI installation and authentication
#    - Integrated with cloud.csv for session management
#
# 6. "codex" (for default codex backend) - Use Codex client
#    - Default OpenAI Codex integration

# Backend Aliases
# ---------------
# You can create multiple configurations for the same backend type by using
# different backend names with the same backend_type. This is useful for:
# - Using multiple models from the same provider
# - Different configurations for different use cases
# - Fallback configurations with different models

# Fallback Backend Strategy
# --------------------------
# The `backend_for_failed_pr` section provides a strategic fallback mechanism:
# 1. When a PR fails to be processed by the primary backends
# 2. The system automatically attempts to process using the fallback backend
# 3. This ensures high availability and resilience in PR automation
# 4. The fallback backend can use a different model or provider for redundancy

# Configuration Inheritance and Options
# -------------------------------------
# 1. The `options` field is used by backends to pass command-line arguments
#    during code editing operations (code modifications, file writes, etc.).
#
# 2. The `options_for_noedit` field is used specifically for message generation
#    operations (commit messages, PR descriptions, etc.). If not specified,
#    it defaults to the same value as `options`.
#
# 3. The `backend_type` field allows you to create custom aliases that
#    map to underlying backend implementations. This is useful when you
#    want multiple configurations for the same backend type.
#
# 4. Different backends use these fields differently:
#    - qwen/codex: Command-line arguments for CLI tools
#    - claude/gemini/auggie: Backend-specific flags and options
#    - jules: Minimal options (session-based backend)

# Environment Variables
# --------------------
# Environment variables can override configuration values:
#    - AUTO_CODER_DEFAULT_BACKEND
#    - AUTO_CODER_BACKEND_FOR_NOEDIT_DEFAULT_BACKEND (or AUTO_CODER_MESSAGE_DEFAULT_BACKEND for backward compatibility)
#    - AUTO_CODER_<BACKEND>_API_KEY (e.g., AUTO_CODER_GEMINI_API_KEY)
#    - AUTO_CODER_<BACKEND>_OPENAI_API_KEY (e.g., AUTO_CODER_CODEX_OPENAI_API_KEY)
#    - AUTO_CODER_<BACKEND>_OPENAI_BASE_URL

# Provider-Specific Configurations
# --------------------------------
# 1. OpenAI-compatible providers (OpenRouter, Azure OpenAI, custom endpoints)
#    should use:
#    - openai_api_key
#    - openai_base_url
#    - backend_type = "codex"
#
# 2. Native providers (Gemini, Claude)
#    should use:
#    - api_key
#    - base_url
#    - backend_type matching the provider name
