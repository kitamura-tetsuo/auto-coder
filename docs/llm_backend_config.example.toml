# LLM Backend Configuration Example
# =================================
#
# This file demonstrates how to configure LLM backends for Auto-Coder.
# Save this file as `~/.auto-coder/llm_config.toml` or specify a custom path.

[backend]
# Default backend to use when multiple are available
default = "qwen-openrouter"

# Order of backends to try (only enabled backends will be used)
order = ["qwen-openrouter", "qwen", "gemini", "codex", "claude"]

[message_backend]
# Separate configuration for message generation (commit messages, PR descriptions)
# If not specified, falls back to general backend configuration
default = "qwen-openrouter"
order = ["qwen-openrouter", "qwen", "gemini"]

[backends]

# Backend Configuration
# =====================
# Note: enabled = true is the default behavior for all backends.
# Only specify "enabled = false" to disable a backend.
# Only the following backends are disabled by default (optional services):
# - qwen-azure: Azure OpenAI (optional)
# - qwen-custom: Custom OpenAI-compatible endpoint (optional)

# Qwen Configuration - Two Methods
# =================================

# Method 1: Qwen CLI (OAuth) - Direct Qwen Usage
# ----------------------------------------------
# Use backend_type = "qwen" for native Qwen CLI authentication
[qwen]
# enabled = true is the default (no need to specify)
model = "qwen3-coder-plus"

# No API keys needed - uses OAuth with Qwen CLI
# Note: This requires Qwen CLI to be installed and authenticated

# Backend type identifier - "qwen" for native CLI
backend_type = "qwen"

# Additional options to pass to the Qwen CLI
# These options will be added to the command line when calling qwen
# Example: ["-o", "stream", "false", "--debug"]
options = []


# Method 2: Qwen via OpenRouter (OpenAI-Compatible API)
# -----------------------------------------------------
# Use backend_type = "codex" for OpenAI-compatible providers
[qwen-openrouter]
# enabled = true is the default (no need to specify)
model = "qwen/qwen3-coder:free"

# OpenAI-compatible API settings (required for OpenRouter, Azure, etc.)
openai_api_key = "sk-or-v1-your-openrouter-key-here"
openai_base_url = "https://openrouter.ai/api/v1"

# Backend type identifier - "codex" for OpenAI-compatible client
backend_type = "codex"

# Optional: Add custom headers for tracking usage
options = ["-o", "HTTPReferer", "https://yourapp.com", "-o", "XTitle", "Auto-Coder"]

# Gemini Configuration
# --------------------
[gemini]
# enabled = true is the default (no need to specify)
model = "gemini-2.5-pro"

# Gemini API settings
api_key = "your-gemini-api-key-here"
base_url = "https://generativelanguage.googleapis.com"

# Optional retry configuration for usage limits
usage_limit_retry_count = 3
usage_limit_retry_wait_seconds = 30
# Rotate to the next backend after every successful call to stay under strict execution limits
always_switch_after_execution = true

backend_type = "gemini"

# Codex Configuration
# -------------------
# Codex client can be used for generic OpenAI-compatible APIs like OpenRouter.
[codex]
# enabled = true is the default (no need to specify)
model = "codex" # Placeholder, model is usually set by the provider
# Generic API key and base URL for providers that don't use OpenAI-specific env vars
api_key = "your-api-key-here"
base_url = "your-api-base-url-here"
# For OpenAI-compatible APIs (like OpenRouter), use these
openai_api_key = "your-openai-api-key-here"
openai_base_url = "your-openai-base-url-here"
options = []
backend_type = "codex"

# Custom Alias Examples
# --------------------
# You can create custom aliases that point to underlying backend types
# This allows you to have multiple configurations for the same backend

# Example 1: Qwen via OpenRouter with faster model
[qwen-fast]
# enabled = true is the default (no need to specify)
model = "qwen/qwen-2.5-plus"
openai_api_key = "sk-or-v1-your-openrouter-key-here"
openai_base_url = "https://openrouter.ai/api/v1"

# Use 'codex' backend type (OpenAI-compatible)
backend_type = "codex"

# Custom options for faster responses
options = ["-o", "timeout", "30", "-o", "stream", "true"]


# Example 2: Qwen via OpenRouter with premium model
[qwen-premium]
# enabled = true is the default (no need to specify)
model = "qwen/qwen-2.5-coder-72b-instruct"
openai_api_key = "sk-or-v1-your-openrouter-key-here"
openai_base_url = "https://openrouter.ai/api/v1"

# Use 'codex' backend type (OpenAI-compatible)
backend_type = "codex"

# Premium model with additional options
options = ["-o", "HTTPReferer", "https://yourapp.com", "-o", "XTitle", "Auto-Coder"]


# Example 3: Azure OpenAI with Qwen model
# NOTE: Disabled by default (optional service)
[qwen-azure]
enabled = false
model = "qwen-35-coder"
openai_api_key = "your-azure-openai-key"
openai_base_url = "https://your-resource.openai.azure.com"

# Use 'codex' backend type for Azure OpenAI
backend_type = "codex"

# Azure-specific options
options = ["-o", "api_version", "2024-02-01"]


# Example 4: Custom OpenAI-compatible endpoint
# NOTE: Disabled by default (optional service)
[qwen-custom]
enabled = false
model = "qwen-2.5-coder-32k-instruct"
openai_api_key = "your-custom-endpoint-key"
openai_base_url = "https://api.example.com/v1"

# Use 'codex' backend type for OpenAI-compatible providers
backend_type = "codex"

# Fallback Backend for Failed PRs
# --------------------------------
# The `backend_for_failed_pr` section allows you to specify a fallback backend
# to use when PR processing fails. This is useful for resilience and ensuring
# that PR processing can continue even when the default backend is unavailable.
#
# If a PR fails to be processed by the primary backends, the system will
# automatically try the fallback backend specified in this section.
#
# Example configuration:
# [backend_for_failed_pr]
# # Backend should be enabled (default: true)
# enabled = true
#
# # Model to use for the fallback backend
# model = "gemini-2.5-flash"
#
# # API key (can also be set via environment variable)
# api_key = "your-fallback-api-key"
#
# # Optional: Override default temperature (0.0 to 1.0)
# temperature = 0.2
#
# # Optional: Override default timeout (in seconds)
# timeout = 120
#
# # Optional: Backend type (e.g., "codex", "gemini", "qwen")
# backend_type = "gemini"
#
# # Optional: List of providers for this backend
# providers = ["google"]
#
# # Optional: OpenAI-compatible configuration
# openai_api_key = "your-openai-key"
# openai_base_url = "https://api.openai.com/v1"
#
# # Optional: Usage limit retry configuration
# usage_limit_retry_count = 3
# usage_limit_retry_wait_seconds = 30
#
# # Optional: Additional options
# options = ["stream", "json_mode"]
#
# # Optional: Model provider (e.g., "openrouter", "anthropic", "google")
# model_provider = "google"
#
# # Optional: Always switch to next backend after execution
# always_switch_after_execution = false
#
# # Optional: Path to settings file (for Claude backend)
# settings = "path/to/settings.json"
#
# # Optional: Usage markers for tracking
# usage_markers = ["fallback", "high_availability"]


# Notes and Best Practices
# ========================

# IMPORTANT: backend_type Configuration
# ---------------------------------------
# The `backend_type` field is crucial for proper configuration:
#
# 1. "qwen" - Use native Qwen CLI (OAuth authentication)
#    - No API keys required
#    - Requires Qwen CLI installation and authentication
#    - Limited to models available through Qwen CLI
#
# 2. "codex" - Use OpenAI-compatible client (API keys required)
#    - Required for OpenRouter, Azure OpenAI, and similar providers
#    - Requires openai_api_key and openai_base_url
#    - Access to all models offered by the provider
#
# 3. "gemini" - Use Gemini client
#    - Requires api_key and base_url
#    - Direct Google API integration
#
# 4. "claude" - Use Claude client
#    - Anthropic API integration
#
# 5. "jules" - Use Jules AI assistant
#    - Session-based AI assistant
#    - Requires Jules CLI installation and authentication
#    - Integrated with cloud.csv for session management
#
# 6. "codex" (for default codex backend) - Use Codex client
#    - Default OpenAI Codex integration

# Backend Aliases
# ---------------
# You can create multiple configurations for the same backend type by using
# different backend names with the same backend_type. This is useful for:
# - Using multiple models from the same provider
# - Different configurations for different use cases
# - Fallback configurations with different models

# Fallback Backend Strategy
# --------------------------
# The `backend_for_failed_pr` section provides a strategic fallback mechanism:
# 1. When a PR fails to be processed by the primary backends
# 2. The system automatically attempts to process using the fallback backend
# 3. This ensures high availability and resilience in PR automation
# 4. The fallback backend can use a different model or provider for redundancy

# Configuration Inheritance and Options
# -------------------------------------
# 1. The `options` field is primarily used by qwen/codex backends to pass
#    command-line arguments to the CLI tools.
#
# 2. Only qwen and codex backends currently utilize the `options` field in their
#    subprocess calls. Other backends may ignore this field.
#
# 3. The `backend_type` field allows you to create custom aliases that
#    map to underlying backend implementations. This is useful when you
#    want multiple configurations for the same backend type.

# Environment Variables
# --------------------
# Environment variables can override configuration values:
#    - AUTO_CODER_DEFAULT_BACKEND
#    - AUTO_CODER_MESSAGE_DEFAULT_BACKEND
#    - AUTO_CODER_<BACKEND>_API_KEY (e.g., AUTO_CODER_GEMINI_API_KEY)
#    - AUTO_CODER_<BACKEND>_OPENAI_API_KEY (e.g., AUTO_CODER_CODEX_OPENAI_API_KEY)
#    - AUTO_CODER_<BACKEND>_OPENAI_BASE_URL

# Provider-Specific Configurations
# --------------------------------
# 1. OpenAI-compatible providers (OpenRouter, Azure OpenAI, custom endpoints)
#    should use:
#    - openai_api_key
#    - openai_base_url
#    - backend_type = "codex"
#
# 2. Native providers (Gemini, Claude)
#    should use:
#    - api_key
#    - base_url
#    - backend_type matching the provider name
