# Client Features Documentation

This file documents the client-facing features and tools provided by auto-coder.

## GraphRAG MCP Server (Customized Fork)

Since auto-coder v2025.10.23, a customized GraphRAG MCP server is bundled with the package. This fork of [rileylemm/graphrag_mcp](https://github.com/rileylemm/graphrag_mcp) has been specialized for code analysis.

### Location

- **Source**: mcp/graphrag_mcp/
- **Documentation**: `docs/BUNDLED_MCP_SERVER.md`
- **Fork Info**: `src/auto_coder/mcp_servers/graphrag_mcp/FORK_INFO.md`

### Purpose

This fork provides specialized TypeScript/JavaScript code analysis capabilities using TypeScript code structure indexed by ts-morph, replacing the original documentation search functionality.

### Key Modifications from Original

1. **Specialized for Code Analysis**: Changed from generic documentation search to TypeScript/JavaScript code structure analysis
2. **Custom Graph Schema**: Adapted to work with ts-morph generated graph structure with node types like `File`, `Function`, `Method`, `Class`, `Interface`, and relationship types like `CONTAINS`, `CALLS`, `EXTENDS`, `IMPLEMENTS`, `IMPORTS`
3. **Code-Specific Tools**: Replaced original `search_documentation` and `hybrid_search` with code analysis tools

### Configuration

#### Backend Configuration

The auto-coder system supports multiple LLM backends:

```toml
          [qwen.qwen-direct]
          command = "uvx"
          args = ["qwen-direct"]
          description = "Direct Qwen API access"
          QWEN_API_KEY = "your-api-key"
```

#### Backend Configuration Fields

The system supports extensive backend configuration options. Key fields include:

- **model_provider**: Specifies the model provider for backends using `backend_type = "codex"`
  - Purpose: Allows users to configure providers like OpenRouter, Anthropic, etc.
  - Example:
    ```toml
    [grok-4.1-fast]
    enabled = true
    model = "x-ai/grok-4.1-fast:free"
    backend_type = "codex"
    model_provider = "openrouter"
    ```
  - This translates to: `codex -c model="x-ai/grok-4.1-fast:free" -c model_provider=openrouter`
  - Optional field: Backward compatible with existing configurations

- **usage_markers**: Configurable strings to detect rate limits or usage errors in CLI output
  - Purpose: Enables users to handle different localizations or new error messages without modifying source code
  - Supported by: ClaudeClient, GeminiClient, CodexClient, AuggieClient, QwenClient
  - Default behavior: Each client has built-in default markers (e.g., "rate limit", "quota", "429")
  - Example:
    ```toml
    [backends.gemini]
    model = "gemini-2.5-pro"
    usage_markers = ["rate limit", "quota exceeded", "custom error message"]

    [backends.claude]
    model = "sonnet"
    usage_markers = ["api error: 429", "usage limit exceeded", "5-hour limit reached"]

    [backends.codex]
    model = "codex"
    usage_markers = ["rate limit", "usage limit", "upgrade to pro"]

    [backends.auggie]
    model = "GPT-5"
    usage_markers = ["rate limit", "quota", "429"]

    [backends.qwen]
    model = "qwen3-coder-plus"
    usage_markers = ["rate limit", "quota", "too many requests", "custom marker"]
    ```
  - Behavior:
    - When configured, the specified markers are used to detect usage limit errors
    - When not configured, clients fall back to their built-in default markers
    - Markers are matched case-insensitively against the CLI output
  - Configuration file: "~/.auto-coder/llm_config.toml"
  - Optional field: Backward compatible with existing configurations

- **backend_for_failed_pr**: Fallback backend configuration for PRs when attempt count reaches 3
  - Purpose: Automatically switches to a different backend when a PR has been attempted 3 times without success
  - Trigger: When any linked issue in a PR has an attempt count >= 3, the system switches to this fallback backend for all subsequent LLM operations on that PR
  - Configuration section: `[backend_for_failed_pr]` in the TOML config file
  - Example:
    ```toml
    [backend_for_failed_pr]
    name = "gemini"
    model = "gemini-2.5-flash"
    api_key = "your-api-key"
    ```
  - Behavior:
    - Checks attempt count for all issues linked in the PR body (using keywords like "close", "fix", "resolve")
    - Uses max attempt count across all linked issues
    - When max attempt >= 3, switches to the fallback backend before making any LLM calls
    - Supports all standard backend configuration fields (model, api_key, base_url, etc.)
    - If no fallback is configured, processing continues with the current backend (no error)
  - Configuration file: "~/.auto-coder/llm_config.toml"
  - Optional field: Only used when automatic fallback is desired

- **`options` field**: CLI options for code editing operations
  - Purpose: Pass additional command-line arguments to backend CLIs when performing code editing operations
  - Type: List[str]
  - Default: `[]` (empty list)
  - Usage: Applied when the LLM is invoked for analyzing issues, generating code, fixing bugs, and other file-modifying operations
  - Example:
    ```toml
    [backends.codex]
    model = "codex"
    options = ["--dangerously-bypass-approvals-and-sandbox"]
    ```
  - Behavior:
    - The system automatically adds required flags for each backend type during execution (e.g., `--dangerously-bypass-approvals-and-sandbox` for Codex)
    - Custom options specified here are appended to the automatically-added flags
    - Different backends may interpret options differently
  - Configuration file: "~/.auto-coder/llm_config.toml"
  - Optional field: Backward compatible with existing configurations

- **`options_for_noedit` field**: CLI options for message generation (non-editing operations)
  - Purpose: Pass additional command-line arguments to backend CLIs when generating commit messages, PR descriptions, and other non-code operations
  - Type: List[str]
  - Default: `[]` (empty list)
  - Usage: Applied when the LLM is invoked for message generation operations (not code editing)
  - Example:
    ```toml
    [backends.gemini]
    model = "gemini-2.5-pro"
    options = ["--debug"]
    options_for_noedit = ["--silent"]
    ```
  - Behavior:
    - If not specified or empty, falls back to using `options` value
    - Allows different configurations for editing vs message generation
    - The system automatically adds required flags for each backend type during execution
    - Useful for optimizing different operations with different option sets
  - Configuration file: "~/.auto-coder/llm_config.toml"
  - Optional field: Backward compatible with existing configurations

- **`backend_for_noedit` configuration**: Separate backend configuration for non-editing operations
  - Purpose: Configure a different backend (or backend order) for message generation compared to code editing
  - Breaking Change: Renamed from `message_backend` to `backend_for_noedit`
  - Configuration Sections:
    - `[backend_for_noedit]`: Top-level section for non-editing backend configuration
    - Fields:
      - `default`: Default backend name for non-editing operations
      - `order`: List of backend names to try for non-editing operations (in order)
  - Example:
    ```toml
    [backend]
    default = "qwen"
    order = ["qwen", "gemini", "claude"]

    [backend_for_noedit]
    default = "claude"
    order = ["claude", "qwen"]
    ```
  - Behavior:
    - If not specified, falls back to general backend configuration (`[backend]` section)
    - Allows optimization: use fast/cheap models for messages, premium models for code
    - Backward compatible: old `message_backend` key still works but emits deprecation warning
  - Migration:
    - Replace `[message_backend]` with `[backend_for_noedit]` in config files
    - Replace environment variable `AUTO_CODER_MESSAGE_DEFAULT_BACKEND` with `AUTO_CODER_NOEDIT_DEFAULT_BACKEND`
  - Configuration file: "~/.auto-coder/llm_config.toml"
  - Optional field: Defaults to general backend configuration if not specified

#### Backend Management

    nested_managers:
      description: "Multiple backend manager instances can exist with different configurations"
      llm_backend: "Singleton instance for general LLM operations (PR processing, test fixes, code generation)"
      noedit_backend: "Separate singleton instance for non-editing operations (commit messages, PR messages)"
      usage: "Access via get_llm_backend_manager() and get_noedit_backend_manager() for different use cases"
      provider_manager: "BackendProviderManager is shared across backend manager instances and tracks provider rotation state"
      deprecated_names: "get_message_backend_manager(), run_llm_message_prompt(), get_message_backend_and_model() are deprecated - use get_noedit_backend_manager(), run_llm_noedit_prompt(), get_noedit_backend_and_model() instead"
  session_resume:
      description: "Persist and reuse backend session identifiers so supported backends can resume previous conversations."
      prerequisites:
        - "Backend configuration includes options_for_resume with a [sessionId] placeholder that matches the backend CLI flag"
      behavior:
        - "Captures session IDs from backend clients after each execution and persists them to ~/.auto-coder/backend_session_state.json"
        - "Automatically injects configured resume options when the same backend runs consecutively and a session ID is available"
        - "Clears persisted session data when rotating to a different backend to avoid cross-backend leakage"
      configuration_file: "~/.auto-coder/llm_config.toml"
      persistence_file: "~/.auto-coder/backend_session_state.json"

  retry_configuration:
      description: "Configurable retry mechanism for LLM backends when usage limits are hit"
      fields:
        - name: "usage_limit_retry_count"
          type: "int"
          default: 0
          purpose: "Number of retry attempts before switching to next backend"
        - name: "usage_limit_retry_wait_seconds"
          type: "int"
          default: 0
          purpose: "Seconds to wait between retry attempts"
      behavior:
        - "When AutoCoderUsageLimitError is caught, backend manager checks retry configuration"
        - "Retries the same backend up to configured count with specified wait time between attempts"
        - "After retries are exhausted, rotates to next available backend"
        - "Default behavior (0 retries, 0 wait) maintains backward compatibility with immediate rotation"
      example:
        toml_example: |
          [backends.gemini]
          model = "gemini-2.5-pro"
          usage_limit_retry_count = 3
          usage_limit_retry_wait_seconds = 30
      configuration_file: "~/.auto-coder/llm_config.toml"
      scope: "Per-backend configuration allows different backends to have different retry policies"
  post_execution_rotation:
      description: "Optional round-robin style rotation after every successful backend call"
      fields:
        - name: "always_switch_after_execution"
          type: "bool"
          default: false
          purpose: "If true, immediately switch to the next backend in backend.order after a successful execution"
      use_cases:
        - "Distribute traffic evenly across providers"
        - "Avoid soft per-backend limits by rotating proactively"
        - "Comply with backends that only permit a single execution per window"
      example:
        toml_example: |
          [backend]
          order = ["gemini", "qwen", "claude"]

          [backends.gemini]
          model = "gemini-2.5-pro"
          always_switch_after_execution = true

          [backends.qwen]
          model = "qwen3-coder-plus"
          always_switch_after_execution = true
      configuration_file: "~/.auto-coder/llm_config.toml"
      scope: "Per-backend flag; defaults to false when omitted"

#### Per-Backend Option Examples

The following examples demonstrate typical `options` and `options_for_noedit` configurations for each supported backend:

- **Codex Backend** (`backend_type = "codex"`):
  ```toml
  [codex]
  model = "codex"
  options = ["--dangerously-bypass-approvals-and-sandbox"]
  options_for_noedit = ["--dangerously-bypass-approvals-and-sandbox"]
  ```
  - Typical use: OpenAI-compatible providers (OpenRouter, Azure OpenAI, custom endpoints)
  - Automatic flags: `--dangerously-bypass-approvals-and-sandbox` during code editing operations
  - Custom options: Add tracking headers, timeouts, or provider-specific flags

- **Claude Backend** (`backend_type = "claude"`):
  ```toml
  [claude]
  model = "sonnet"
  backend_type = "claude"
  options = []
  options_for_noedit = []
  ```
  - Typical use: Anthropic Claude API
  - Automatic flags: `--print`, `--dangerously-skip-permissions`, `--allow-dangerously-skip-permissions`
  - Custom options: Path to settings file, debug modes

- **Gemini Backend** (`backend_type = "gemini"`):
  ```toml
  [gemini]
  model = "gemini-2.5-pro"
  backend_type = "gemini"
  options = []
  options_for_noedit = []
  ```
  - Typical use: Google Gemini API
  - Automatic flags: `--yolo`, `--force-model`
  - Custom options: Debug modes, output formatting

- **Qwen Backend** (`backend_type = "qwen"`):
  ```toml
  [qwen]
  model = "qwen3-coder-plus"
  backend_type = "qwen"
  options = []
  options_for_noedit = []
  ```
  - Typical use: Native Qwen CLI with OAuth authentication
  - Automatic flags: `-y` (auto-confirm)
  - Custom options: Stream settings, debug modes, timeouts

- **Qwen via OpenRouter** (`backend_type = "codex"`):
  ```toml
  [qwen-openrouter]
  model = "qwen/qwen3-coder:free"
  backend_type = "codex"
  openai_api_key = "sk-or-v1-your-key"
  openai_base_url = "https://openrouter.ai/api/v1"
  options = ["-o", "HTTPReferer", "https://yourapp.com", "-o", "XTitle", "Auto-Coder"]
  options_for_noedit = ["-o", "timeout", "30"]
  ```
  - Typical use: Qwen models through OpenAI-compatible API
  - Automatic flags: `--dangerously-bypass-approvals-and-sandbox`
  - Custom options: Tracking headers, timeouts, stream settings

- **Auggie Backend** (`backend_type = "auggie"`):
  ```toml
  [auggie]
  model = "GPT-5"
  backend_type = "auggie"
  options = []
  options_for_noedit = []
  ```
  - Typical use: AugmentCode Auggie CLI
  - Automatic flags: `--print`
  - Custom options: Debug modes, output formatting

- **Jules Backend** (`backend_type = "jules"`):
  ```toml
  [jules]
  backend_type = "jules"
  options = []
  options_for_noedit = []
  ```
  - Typical use: Session-based AI assistant (Jules)
  - Automatic flags: Minimal - Jules manages sessions internally
  - Custom options: Usually minimal due to session-based nature

#### Breaking Changes Documentation

Recent configuration schema changes that require migration:

**1. Configuration Field Renames**:
   - **`message_backend` → `backend_for_noedit`**:
     - Old config:
       ```toml
       [message_backend]
       default = "claude"
       order = ["claude", "qwen"]
       ```
     - New config:
       ```toml
       [backend_for_noedit]
       default = "claude"
       order = ["claude", "qwen"]
       ```
     - Migration: Update all config files to use new field name
     - Deprecation: Old name still works but emits warning and will be removed in future version

**2. Environment Variable Renames**:
   - **`AUTO_CODER_MESSAGE_DEFAULT_BACKEND` → `AUTO_CODER_NOEDIT_DEFAULT_BACKEND`**:
     - Migration: Update environment variables to new name
     - Deprecation: Old name still works but emits warning

**3. Hardcoded Options Removal**:
   - **Before**: CLI options were hardcoded in client implementations
     - Codex: always used `--dangerously-bypass-approvals-and-sandbox`
     - Claude: always used `--print --dangerously-skip-permissions --allow-dangerously-skip-permissions`
     - Gemini: always used `--yolo --force-model`
     - Qwen: always used `-y`
   - **After**: Options are configurable through `options` and `options_for_noedit` fields
     - System automatically adds required flags for each backend
     - Users can add custom options beyond the defaults
     - Allows fine-tuning per-operation-type behavior
   - Migration: No action required - configuration is backward compatible
   - Benefits: Full customization, better separation of concerns

**4. Default Option Behavior**:
   - When `options_for_noedit` is not specified, it now falls back to `options` value
   - Previously, each backend had different default behaviors
   - Migration: No action required - existing configs work as expected
   - Note: Empty list `[]` means use defaults, not "no options"

#### GraphRAG Integration

  graphrag_integration:
    auto_setup: true
    mcp_config_check: true
    index_update: available via CLI
    snapshot_cleanup:
      retention_days_default: 7
      max_snapshots_per_repo_default: 9
      env_flags:
        - GRAPHRAG_RETENTION_DAYS
        - GRAPHRAG_MAX_SNAPSHOTS_PER_REPO
        - GRAPHRAG_CLEANUP_ON_INIT
        - GRAPHRAG_CLEANUP_ON_UPDATE
      cli_command: "auto-coder graphrag cleanup [--dry-run] [--retention-days N] [--max-per-repo M] [--repo-path PATH]"
      hooks:
        - initialize_graphrag
        - GraphRAGIndexManager.update_index
    pytest_fallback: "Under pytest, GraphRAGIndexManager avoids external graph-builder and uses fast Python fallback indexing"
    docker_manager:
      fallback_recovery: true
      external_network: true
      auto_retry_on_failure: true

#### Test Watcher

  test_watcher:
    playwright_safety:
      - "Skips Playwright execution under pytest or when AC_DISABLE_PLAYWRIGHT=1; returns synthetic report"
      - "Quick availability probe: 'npx playwright --version' with 5s timeout"
      - "Process timeout: 120s communicate() timeout; force-kill on timeout"
    watchdog_daemon: "Observer thread is daemonized; does not block interpreter exit"
    concurrency:
      - "Run-on-change spawns daemon threads for Playwright and GraphRAG update"
      - "GraphRAG retry timers are daemonized"
      - "GraphRAG index updates in TestWatcherTool are scoped to project_root, keeping pytest flows fast on temporary project trees"

### Available Tools

The following code analysis tools are available through the MCP server:

#### find_symbol
Find code symbols by fully qualified name in the codebase.

#### get_call_graph
Analyze function/method call relationships to understand code flow.

#### get_dependencies
Analyze file dependencies to understand project structure.

#### impact_analysis
Analyze change impact across the codebase to understand how a change might affect other parts of the system.

#### semantic_code_search
Search for code using semantic understanding rather than just text matching.

### Setup

The MCP server is automatically set up when running:

```bash
auto-coder graphrag setup-mcp
```

This command copies the bundled server from `src/auto_coder/mcp_servers/graphrag_mcp/` to `~/graphrag_mcp/` and starts the Docker container.

### Usage

After setup, the tools can be accessed through the auto-coder CLI when the MCP server is running.

### Fork Attribution

This fork maintains full attribution to the original author Riley Lemm. The original repository is https://github.com/rileylemm/graphrag_mcp and uses the MIT License.

## Attempt Mechanism

  attempt_management:
    description: "Tracks retries per issue/PR and aligns work branches to the current attempt."
    tracking:
      - "Attempts are recorded as standardized comments 'Auto-Coder Attempt: <N>' with an optional detail suffix."
      - "get_current_attempt reads the latest attempt number from those comments (legacy timestamped comments stay compatible)."
    branching:
      - "Work branches follow issue-<number> for the first pass and issue-<number>/attempt-<N> for subsequent passes; parent branches cascade to sub-issues."
      - "When the local branch is behind the recorded attempt, Auto-Coder switches or recreates the correct attempt branch before continuing work."
      - "New attempt branches are created from the validated base branch (main or the parent's current attempt branch) to re-implement changes safely."
    fallback:
      - "PR failures that cannot be auto-merged (LLM CANNOT_FIX/unclear output, commit/push errors, failed merges or conflict resolution) trigger attempt increments for every linked issue."
      - "Conflict resolver fallbacks do the same when LLM-based conflict handling leaves unresolved markers or cannot push a clean merge."
      - "When attempt count reaches 3 for any linked issue, the system automatically switches to the configured fallback backend (see [backend_for_failed_pr] configuration)."
      - "Backend fallback provides a fresh perspective using a different LLM after multiple failed attempts, improving chances of successful PR resolution."
    propagation:
      - "increment_attempt also reopens closed sub-issues and bumps their counters to keep parent/child attempts in sync."
    parent_handling:
      - "When processing a child issue whose parent issue is closed, the system automatically reopens the parent issue before continuing"
      - "This ensures branch selection, base branch selection, and attempt tracking use the parent issue context"
      - "Reopening closed parents maintains proper workflow continuity and prevents branch naming and base selection issues"

## Lock Mechanism

  lock_mechanism:
    description: "Prevents concurrent auto-coder executions to avoid conflicts and data corruption"
    behavior:
      - "Automatically acquires a lock before executing any command (except read-only commands like 'config' and 'unlock')"
      - "Lock files are stored in the repository's .git directory as 'auto-coder.lock'"
      - "Lock information includes PID, hostname, and start time"
      - "Automatically detects stale locks (process no longer running)"
      - "Lock is released when command completes successfully"
      - "Supports context manager protocol for safe lock acquisition and release"
    usage_patterns:
      context_manager:
        description: "Recommended pattern using Python's with statement"
        example: |
          with LockManager() as lock:
              # Lock is automatically acquired here
              do_work()
          # Lock is automatically released here, even if an exception occurs
        benefits:
          - "Automatic cleanup even on exceptions"
          - "More Pythonic and readable code"
          - "Prevents accidental lock leaks"
      manual_management:
        description: "Explicit acquire/release pattern"
        example: |
          lock = LockManager()
          if lock.acquire():
              try:
                  do_work()
              finally:
                  lock.release()
        use_cases:
          - "Complex control flow requiring early release"
          - "Legacy code compatibility"
    commands:
      auto_lock:
        description: "Lock is automatically acquired by default for all commands"
        exceptions: "Read-only commands ('config', 'unlock', 'auth-status', 'get-actions-logs', 'mcp-pdb')"
      unlock:
        description: "Manually remove a lock file"
        usage: "auto-coder lock unlock"
        options:
          - "--force: Force remove lock file even if process appears to be running (use with caution)"
        behavior:
          - "Shows lock information before removing"
          - "Prevents removal if process is still running (unless --force is used)"
          - "Automatically detects and removes stale locks without --force flag"
    error_handling:
      concurrent_execution:
        description: "When another instance is already running"
        message: "Displays error with lock information (PID, hostname, start time, status)"
        action: "User must wait for existing instance to complete or use 'unlock' command"
      stale_lock:
        description: "When lock file exists but process is no longer running"
        detection: "Checks if PID is still active using os.kill(pid, 0) on Unix or OpenProcess on Windows"
        action: "Safe to remove without --force flag"
    technical_details:
      implementation: "LockManager class in src/auto_coder/lock_manager.py"
      cli_commands: "src/auto_coder/cli_commands_lock.py"
      integration: "Lock check in cli.py before command execution"
      skip_conditions: "Temporary directories (paths containing 'tmp' or 'pytest')"
      storage_format: "JSON file with pid, hostname, and started_at fields"
