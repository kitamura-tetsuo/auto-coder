# Client Features Documentation

This file documents the client-facing features and tools provided by auto-coder.

## GraphRAG MCP Server (Customized Fork)

Since auto-coder v2025.10.23, a customized GraphRAG MCP server is bundled with the package. This fork of [rileylemm/graphrag_mcp](https://github.com/rileylemm/graphrag_mcp) has been specialized for code analysis.

### Location

- **Source**: mcp/graphrag_mcp/
- **Documentation**: `docs/BUNDLED_MCP_SERVER.md`
- **Fork Info**: `src/auto_coder/mcp_servers/graphrag_mcp/FORK_INFO.md`

### Purpose

This fork provides specialized TypeScript/JavaScript code analysis capabilities using TypeScript code structure indexed by ts-morph, replacing the original documentation search functionality.

### Key Modifications from Original

1. **Specialized for Code Analysis**: Changed from generic documentation search to TypeScript/JavaScript code structure analysis
2. **Custom Graph Schema**: Adapted to work with ts-morph generated graph structure with node types like `File`, `Function`, `Method`, `Class`, `Interface`, and relationship types like `CONTAINS`, `CALLS`, `EXTENDS`, `IMPLEMENTS`, `IMPORTS`
3. **Code-Specific Tools**: Replaced original `search_documentation` and `hybrid_search` with code analysis tools

### Configuration

#### Backend Configuration

The auto-coder system supports multiple LLM backends:

```toml
          [qwen.qwen-direct]
          command = "uvx"
          args = ["qwen-direct"]
          description = "Direct Qwen API access"
          QWEN_API_KEY = "your-api-key"
```

#### Backend Configuration Fields

The system supports extensive backend configuration options. Key fields include:

- **model_provider**: Specifies the model provider for backends using `backend_type = "codex"`
  - Purpose: Allows users to configure providers like OpenRouter, Anthropic, etc.
  - Example:
    ```toml
    [grok-4.1-fast]
    enabled = true
    model = "x-ai/grok-4.1-fast:free"
    backend_type = "codex"
    model_provider = "openrouter"
    ```
  - This translates to: `codex -c model="x-ai/grok-4.1-fast:free" -c model_provider=openrouter`
  - Optional field: Backward compatible with existing configurations

- **usage_markers**: Configurable strings to detect rate limits or usage errors in CLI output
  - Purpose: Enables users to handle different localizations or new error messages without modifying source code
  - Supported by: ClaudeClient, GeminiClient, CodexClient, AuggieClient, QwenClient
  - Default behavior: Each client has built-in default markers (e.g., "rate limit", "quota", "429")
  - Example:
    ```toml
    [backends.gemini]
    model = "gemini-2.5-pro"
    usage_markers = ["rate limit", "quota exceeded", "custom error message"]

    [backends.claude]
    model = "sonnet"
    usage_markers = ["api error: 429", "usage limit exceeded", "5-hour limit reached"]

    [backends.codex]
    model = "codex"
    usage_markers = ["rate limit", "usage limit", "upgrade to pro"]

    [backends.auggie]
    model = "GPT-5"
    usage_markers = ["rate limit", "quota", "429"]

    [backends.qwen]
    model = "qwen3-coder-plus"
    usage_markers = ["rate limit", "quota", "too many requests", "custom marker"]
    ```
  - Behavior:
    - When configured, the specified markers are used to detect usage limit errors
    - When not configured, clients fall back to their built-in default markers
    - Markers are matched case-insensitively against the CLI output
  - Configuration file: "~/.auto-coder/llm_config.toml"
  - Optional field: Backward compatible with existing configurations

- **backend_for_failed_pr**: Fallback backend configuration for PRs when attempt count reaches 3
  - Purpose: Automatically switches to a different backend when a PR has been attempted 3 times without success
  - Trigger: When any linked issue in a PR has an attempt count >= 3, the system switches to this fallback backend for all subsequent LLM operations on that PR
  - Configuration section: `[backend_for_failed_pr]` in the TOML config file
  - Example:
    ```toml
    [backend_for_failed_pr]
    name = "gemini"
    model = "gemini-2.5-flash"
    api_key = "your-api-key"
    ```
  - Behavior:
    - Checks attempt count for all issues linked in the PR body (using keywords like "close", "fix", "resolve")
    - Uses max attempt count across all linked issues
    - When max attempt >= 3, switches to the fallback backend before making any LLM calls
    - Supports all standard backend configuration fields (model, api_key, base_url, etc.)
    - If no fallback is configured, processing continues with the current backend (no error)
  - Configuration file: "~/.auto-coder/llm_config.toml"
  - Optional field: Only used when automatic fallback is desired

#### Backend Management

    nested_managers:
      description: "Multiple backend manager instances can exist with different configurations"
      llm_backend: "Singleton instance for general LLM operations (PR processing, test fixes, code generation)"
      noedit_backend: "Separate singleton instance for non-editing operations (commit messages, PR messages)"
      usage: "Access via get_llm_backend_manager() and get_noedit_backend_manager() for different use cases"
      provider_manager: "BackendProviderManager is shared across backend manager instances and tracks provider rotation state"
      deprecated_names: "get_message_backend_manager(), run_llm_message_prompt(), get_message_backend_and_model() are deprecated - use get_noedit_backend_manager(), run_llm_noedit_prompt(), get_noedit_backend_and_model() instead"

  retry_configuration:
      description: "Configurable retry mechanism for LLM backends when usage limits are hit"
      fields:
        - name: "usage_limit_retry_count"
          type: "int"
          default: 0
          purpose: "Number of retry attempts before switching to next backend"
        - name: "usage_limit_retry_wait_seconds"
          type: "int"
          default: 0
          purpose: "Seconds to wait between retry attempts"
      behavior:
        - "When AutoCoderUsageLimitError is caught, backend manager checks retry configuration"
        - "Retries the same backend up to configured count with specified wait time between attempts"
        - "After retries are exhausted, rotates to next available backend"
        - "Default behavior (0 retries, 0 wait) maintains backward compatibility with immediate rotation"
      example:
        toml_example: |
          [backends.gemini]
          model = "gemini-2.5-pro"
          usage_limit_retry_count = 3
          usage_limit_retry_wait_seconds = 30
      configuration_file: "~/.auto-coder/llm_config.toml"
      scope: "Per-backend configuration allows different backends to have different retry policies"
  post_execution_rotation:
      description: "Optional round-robin style rotation after every successful backend call"
      fields:
        - name: "always_switch_after_execution"
          type: "bool"
          default: false
          purpose: "If true, immediately switch to the next backend in backend.order after a successful execution"
      use_cases:
        - "Distribute traffic evenly across providers"
        - "Avoid soft per-backend limits by rotating proactively"
        - "Comply with backends that only permit a single execution per window"
      example:
        toml_example: |
          [backend]
          order = ["gemini", "qwen", "claude"]

          [backends.gemini]
          model = "gemini-2.5-pro"
          always_switch_after_execution = true

          [backends.qwen]
          model = "qwen3-coder-plus"
          always_switch_after_execution = true
      configuration_file: "~/.auto-coder/llm_config.toml"
      scope: "Per-backend flag; defaults to false when omitted"

#### GraphRAG Integration

  graphrag_integration:
    auto_setup: true
    mcp_config_check: true
    index_update: available via CLI
    snapshot_cleanup:
      retention_days_default: 7
      max_snapshots_per_repo_default: 9
      env_flags:
        - GRAPHRAG_RETENTION_DAYS
        - GRAPHRAG_MAX_SNAPSHOTS_PER_REPO
        - GRAPHRAG_CLEANUP_ON_INIT
        - GRAPHRAG_CLEANUP_ON_UPDATE
      cli_command: "auto-coder graphrag cleanup [--dry-run] [--retention-days N] [--max-per-repo M] [--repo-path PATH]"
      hooks:
        - initialize_graphrag
        - GraphRAGIndexManager.update_index
    pytest_fallback: "Under pytest, GraphRAGIndexManager avoids external graph-builder and uses fast Python fallback indexing"
    docker_manager:
      fallback_recovery: true
      external_network: true
      auto_retry_on_failure: true

#### Test Watcher

  test_watcher:
    playwright_safety:
      - "Skips Playwright execution under pytest or when AC_DISABLE_PLAYWRIGHT=1; returns synthetic report"
      - "Quick availability probe: 'npx playwright --version' with 5s timeout"
      - "Process timeout: 120s communicate() timeout; force-kill on timeout"
    watchdog_daemon: "Observer thread is daemonized; does not block interpreter exit"
    concurrency:
      - "Run-on-change spawns daemon threads for Playwright and GraphRAG update"
      - "GraphRAG retry timers are daemonized"
      - "GraphRAG index updates in TestWatcherTool are scoped to project_root, keeping pytest flows fast on temporary project trees"

### Available Tools

The following code analysis tools are available through the MCP server:

#### find_symbol
Find code symbols by fully qualified name in the codebase.

#### get_call_graph
Analyze function/method call relationships to understand code flow.

#### get_dependencies
Analyze file dependencies to understand project structure.

#### impact_analysis
Analyze change impact across the codebase to understand how a change might affect other parts of the system.

#### semantic_code_search
Search for code using semantic understanding rather than just text matching.

### Setup

The MCP server is automatically set up when running:

```bash
auto-coder graphrag setup-mcp
```

This command copies the bundled server from `src/auto_coder/mcp_servers/graphrag_mcp/` to `~/graphrag_mcp/` and starts the Docker container.

### Usage

After setup, the tools can be accessed through the auto-coder CLI when the MCP server is running.

### Fork Attribution

This fork maintains full attribution to the original author Riley Lemm. The original repository is https://github.com/rileylemm/graphrag_mcp and uses the MIT License.

## Attempt Mechanism

  attempt_management:
    description: "Tracks retries per issue/PR and aligns work branches to the current attempt."
    tracking:
      - "Attempts are recorded as standardized comments 'Auto-Coder Attempt: <N>' with an optional detail suffix."
      - "get_current_attempt reads the latest attempt number from those comments (legacy timestamped comments stay compatible)."
    branching:
      - "Work branches follow issue-<number> for the first pass and issue-<number>/attempt-<N> for subsequent passes; parent branches cascade to sub-issues."
      - "When the local branch is behind the recorded attempt, Auto-Coder switches or recreates the correct attempt branch before continuing work."
      - "New attempt branches are created from the validated base branch (main or the parent's current attempt branch) to re-implement changes safely."
    fallback:
      - "PR failures that cannot be auto-merged (LLM CANNOT_FIX/unclear output, commit/push errors, failed merges or conflict resolution) trigger attempt increments for every linked issue."
      - "Conflict resolver fallbacks do the same when LLM-based conflict handling leaves unresolved markers or cannot push a clean merge."
      - "When attempt count reaches 3 for any linked issue, the system automatically switches to the configured fallback backend (see [backend_for_failed_pr] configuration)."
      - "Backend fallback provides a fresh perspective using a different LLM after multiple failed attempts, improving chances of successful PR resolution."
    propagation:
      - "increment_attempt also reopens closed sub-issues and bumps their counters to keep parent/child attempts in sync."
    parent_handling:
      - "When processing a child issue whose parent issue is closed, the system automatically reopens the parent issue before continuing"
      - "This ensures branch selection, base branch selection, and attempt tracking use the parent issue context"
      - "Reopening closed parents maintains proper workflow continuity and prevents branch naming and base selection issues"

## Lock Mechanism

  lock_mechanism:
    description: "Prevents concurrent auto-coder executions to avoid conflicts and data corruption"
    behavior:
      - "Automatically acquires a lock before executing any command (except read-only commands like 'config' and 'unlock')"
      - "Lock files are stored in the repository's .git directory as 'auto-coder.lock'"
      - "Lock information includes PID, hostname, and start time"
      - "Automatically detects stale locks (process no longer running)"
      - "Lock is released when command completes successfully"
      - "Supports context manager protocol for safe lock acquisition and release"
    usage_patterns:
      context_manager:
        description: "Recommended pattern using Python's with statement"
        example: |
          with LockManager() as lock:
              # Lock is automatically acquired here
              do_work()
          # Lock is automatically released here, even if an exception occurs
        benefits:
          - "Automatic cleanup even on exceptions"
          - "More Pythonic and readable code"
          - "Prevents accidental lock leaks"
      manual_management:
        description: "Explicit acquire/release pattern"
        example: |
          lock = LockManager()
          if lock.acquire():
              try:
                  do_work()
              finally:
                  lock.release()
        use_cases:
          - "Complex control flow requiring early release"
          - "Legacy code compatibility"
    commands:
      auto_lock:
        description: "Lock is automatically acquired by default for all commands"
        exceptions: "Read-only commands ('config', 'unlock', 'auth-status', 'get-actions-logs', 'mcp-pdb')"
      unlock:
        description: "Manually remove a lock file"
        usage: "auto-coder lock unlock"
        options:
          - "--force: Force remove lock file even if process appears to be running (use with caution)"
        behavior:
          - "Shows lock information before removing"
          - "Prevents removal if process is still running (unless --force is used)"
          - "Automatically detects and removes stale locks without --force flag"
    error_handling:
      concurrent_execution:
        description: "When another instance is already running"
        message: "Displays error with lock information (PID, hostname, start time, status)"
        action: "User must wait for existing instance to complete or use 'unlock' command"
      stale_lock:
        description: "When lock file exists but process is no longer running"
        detection: "Checks if PID is still active using os.kill(pid, 0) on Unix or OpenProcess on Windows"
        action: "Safe to remove without --force flag"
    technical_details:
      implementation: "LockManager class in src/auto_coder/lock_manager.py"
      cli_commands: "src/auto_coder/cli_commands_lock.py"
      integration: "Lock check in cli.py before command execution"
      skip_conditions: "Temporary directories (paths containing 'tmp' or 'pytest')"
      storage_format: "JSON file with pid, hostname, and started_at fields"
