# Client Features Documentation

This file documents the client-facing features and tools provided by auto-coder.

## GraphRAG MCP Server (Customized Fork)

Since auto-coder v2025.10.23, a customized GraphRAG MCP server is bundled with the package. This fork of [rileylemm/graphrag_mcp](https://github.com/rileylemm/graphrag_mcp) has been specialized for code analysis.

### Location

- **Source**: mcp/graphrag_mcp/
- **Documentation**: `docs/BUNDLED_MCP_SERVER.md`
- **Fork Info**: `src/auto_coder/mcp_servers/graphrag_mcp/FORK_INFO.md`

### Purpose

This fork provides specialized TypeScript/JavaScript code analysis capabilities using TypeScript code structure indexed by ts-morph, replacing the original documentation search functionality.

### Key Modifications from Original

1. **Specialized for Code Analysis**: Changed from generic documentation search to TypeScript/JavaScript code structure analysis
2. **Custom Graph Schema**: Adapted to work with ts-morph generated graph structure with node types like `File`, `Function`, `Method`, `Class`, `Interface`, and relationship types like `CONTAINS`, `CALLS`, `EXTENDS`, `IMPLEMENTS`, `IMPORTS`
3. **Code-Specific Tools**: Replaced original `search_documentation` and `hybrid_search` with code analysis tools

### Configuration

#### Backend Configuration

The auto-coder system supports multiple LLM backends:

```toml
          [qwen.qwen-direct]
          command = "uvx"
          args = ["qwen-direct"]
          description = "Direct Qwen API access"
          QWEN_API_KEY = "your-api-key"
```

#### Backend Management

    nested_managers:
      description: "Multiple backend manager instances can exist with different configurations"
      llm_backend: "Singleton instance for general LLM operations (PR processing, test fixes, code generation)"
      message_backend: "Separate singleton instance for message generation (commit messages, PR messages)"
      usage: "Access via get_llm_backend_manager() and get_message_backend_manager() for different use cases"
      provider_manager: "BackendProviderManager is shared across backend manager instances and tracks provider rotation state"

  retry_configuration:
      description: "Configurable retry mechanism for LLM backends when usage limits are hit"
      fields:
        - name: "usage_limit_retry_count"
          type: "int"
          default: 0
          purpose: "Number of retry attempts before switching to next backend"
        - name: "usage_limit_retry_wait_seconds"
          type: "int"
          default: 0
          purpose: "Seconds to wait between retry attempts"
      behavior:
        - "When AutoCoderUsageLimitError is caught, backend manager checks retry configuration"
        - "Retries the same backend up to configured count with specified wait time between attempts"
        - "After retries are exhausted, rotates to next available backend"
        - "Default behavior (0 retries, 0 wait) maintains backward compatibility with immediate rotation"
      example:
        toml_example: |
          [backends.gemini]
          enabled = true
          model = "gemini-2.5-pro"
          usage_limit_retry_count = 3
          usage_limit_retry_wait_seconds = 30
      configuration_file: "~/.auto-coder/llm_config.toml"
      scope: "Per-backend configuration allows different backends to have different retry policies"

#### GraphRAG Integration

  graphrag_integration:
    auto_setup: true
    mcp_config_check: true
    index_update: available via CLI
    snapshot_cleanup:
      retention_days_default: 7
      max_snapshots_per_repo_default: 9
      env_flags:
        - GRAPHRAG_RETENTION_DAYS
        - GRAPHRAG_MAX_SNAPSHOTS_PER_REPO
        - GRAPHRAG_CLEANUP_ON_INIT
        - GRAPHRAG_CLEANUP_ON_UPDATE
      cli_command: "auto-coder graphrag cleanup [--dry-run] [--retention-days N] [--max-per-repo M] [--repo-path PATH]"
      hooks:
        - initialize_graphrag
        - GraphRAGIndexManager.update_index
    pytest_fallback: "Under pytest, GraphRAGIndexManager avoids external graph-builder and uses fast Python fallback indexing"
    docker_manager:
      fallback_recovery: true
      external_network: true
      auto_retry_on_failure: true

#### Test Watcher

  test_watcher:
    playwright_safety:
      - "Skips Playwright execution under pytest or when AC_DISABLE_PLAYWRIGHT=1; returns synthetic report"
      - "Quick availability probe: 'npx playwright --version' with 5s timeout"
      - "Process timeout: 120s communicate() timeout; force-kill on timeout"
    watchdog_daemon: "Observer thread is daemonized; does not block interpreter exit"
    concurrency:
      - "Run-on-change spawns daemon threads for Playwright and GraphRAG update"
      - "GraphRAG retry timers are daemonized"
      - "GraphRAG index updates in TestWatcherTool are scoped to project_root, keeping pytest flows fast on temporary project trees"

### Available Tools

The following code analysis tools are available through the MCP server:

#### find_symbol
Find code symbols by fully qualified name in the codebase.

#### get_call_graph
Analyze function/method call relationships to understand code flow.

#### get_dependencies
Analyze file dependencies to understand project structure.

#### impact_analysis
Analyze change impact across the codebase to understand how a change might affect other parts of the system.

#### semantic_code_search
Search for code using semantic understanding rather than just text matching.

### Setup

The MCP server is automatically set up when running:

```bash
auto-coder graphrag setup-mcp
```

This command copies the bundled server from `src/auto_coder/mcp_servers/graphrag_mcp/` to `~/graphrag_mcp/` and starts the Docker container.

### Usage

After setup, the tools can be accessed through the auto-coder CLI when the MCP server is running.

### Fork Attribution

This fork maintains full attribution to the original author Riley Lemm. The original repository is https://github.com/rileylemm/graphrag_mcp and uses the MIT License.

## Attempt Mechanism

  attempt_management:
    description: "Tracks retries per issue/PR and aligns work branches to the current attempt."
    tracking:
      - "Attempts are recorded as standardized comments 'Auto-Coder Attempt: <N>' with an optional detail suffix."
      - "get_current_attempt reads the latest attempt number from those comments (legacy timestamped comments stay compatible)."
    branching:
      - "Work branches follow issue-<number> for the first pass and issue-<number>/attempt-<N> for subsequent passes; parent branches cascade to sub-issues."
      - "When the local branch is behind the recorded attempt, Auto-Coder switches or recreates the correct attempt branch before continuing work."
      - "New attempt branches are created from the validated base branch (main or the parent's current attempt branch) to re-implement changes safely."
    fallback:
      - "PR failures that cannot be auto-merged (LLM CANNOT_FIX/unclear output, commit/push errors, failed merges or conflict resolution) trigger attempt increments for every linked issue."
      - "Conflict resolver fallbacks do the same when LLM-based conflict handling leaves unresolved markers or cannot push a clean merge."
    propagation:
      - "increment_attempt also reopens closed sub-issues and bumps their counters to keep parent/child attempts in sync."
